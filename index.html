<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Eazy3DTrack-MV</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Easy3DReT: Uncalibrated Multi-view Multi-Human 3D Tracking & Reconstruction</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!--
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
                  </div>
                -->
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">WACV 2025 Submission: </span>
                    <!--
                      <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    -->
                    </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                         <!--
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  -->
                    <!-- Supplementary PDF link -->
                    <!--
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>
                  -->
                  <!-- Github link -->
                  <!--
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>-->

                <!-- ArXiv abstract Link -->
                <!--
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container">
      <!--<h2 class="title is-3 has-text-centered">Method Overview</h2>-->
    
    <div class="hero-body">
      <img src="static/images/ECCV Teaser.jpg" alt="Teaser Image" style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Multi-view multi-human tracking and reconstruction from uncalibrated cameras. Without knowing the intrinsic and extrinsic camera parameters, our method leverages 3D human priors and ground plane constraints to achieve automatic camera pose calibration and human tracking & reconstruction. Here, we see the reconstruction faithfully reflects the actual location and pose of the people in 3D space.
        </h2>

    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Current methods performing 3D human pose estimation from multi-view still bear several key limitations. 
            First, most methods require manual intrinsic and extrinsic camera calibration, which is laborious and difficult in many settings. 
            Second, more accurate models rely on further training on the same datasets they evaluate, severely limiting their generalizability in real-world settings. 
            We address these limitations with Easy3DReT (Easy REconstruction and Tracking in 3D), which simultaneously reconstructs and tracks 3D humans in a global coordinate frame across all views with uncalibrated cameras and videos in the wild. 
            Easy3DReT is a compositional framework that composes our proposed modules (<i>Automatic Calibration module, Adaptive Stitching Module, and Optimization Module</i>) and off-the-shelf, large pre-trained models at intermediate steps to avoid manual intrinsic and extrinsic calibration and task-specific training.
            Easy3DReT outperforms all existing multi-view 3D tracking or pose estimation methods in Panoptic, EgoHumans, Shelf, and Human3.6M datasets. Code and demos will be released. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
      <h2 class="title is-3 has-text-centered" style="margin-top: 30px;">Overview Video</h2>
    
    <div class="hero-body">
      <!--<img src="static/images/ECCV Teaser.jpg" alt="Teaser Image" style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Multi-view multi-human tracking and reconstruction from uncalibrated cameras. Without knowing the intrinsic and extrinsic camera parameters, our method leverages 3D human priors and ground plane constraints to achieve automatic camera pose calibration and human tracking & reconstruction. Here, we see the reconstruction faithfully reflects the actual location and pose of the people in 3D space.      </h2>-->
        <h2 class="subtitle has-text-justified is-size-4">
          Given an input video, we first extract multi-human tracklets from single-view videos and then achieve automatic camera calibration based on the reconstructed 3D humans. Through iterative global optimization, we jointly refine multi-human tracking, 3D human poses, camera parameters, and ground plane estimation. Extensive experiments show that our method achieves state-of-the-art performance, even when compared with methods using calibrated cameras.
      </h2>
        <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Lego.mp4"
        type="video/mp4">
      </video>
      
      
    </div>
  </div>
</section>
<!-- End teaser video -->

<section class="hero teaser">
  <div class="container">
      <h2 class="title is-3 has-text-centered">Pipeline</h2>
    
    <div class="hero-body">
      <h2 class="subtitle has-text-justified is-size-4">
        We first process every single view independently
        to get 3D human reconstruction and tracking results. Then, we match humans across
        views to estimate camera poses, and aggregate parameters from independent views to a
        global world coordinate, which complements accurate 3D representations of previously
        occluded humans. Finally, we use an iterative global optimization to refine multi-human
        tracking, 3D human poses, camera parameters, and ground plane estimation.       </h2>
      <img src="static/images/ECCV Overview.jpg" alt="Teaser Image" style="max-width: 100%; height: auto;">
      

    </div>
  </div>
</section>

<!-- Image carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">-->
        <!-- Your image here -->
        <!--
        <img src="static/images/Teaser Draft.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Multi-view multi-human tracking and reconstruction from uncalibrated cameras. Without knowing the intrinsic and extrinsic camera parameters, our method leverages 3D human priors and ground plane constraints to achieve automatic camera pose calibration and human tracking & reconstruction. Here, we see the reconstruction faithfully reflects the actual location and pose of the people in 3D space.
        </h2>
      </div>
      <div class="item">-->
        <!-- Your image here -->
        <!--<img src="static/images/Qualitative Results.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results
        </h2>
      </div>
      <div class="item">-->
        <!-- Your image here -->
        <!-- <img src="static/images/id-switch-all.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         ID switch prevention
       </h2>
     </div>
     <div class="item">-->
      <!-- Your image here -->
      <!-- <img src="static/images/CVPR Overview 1.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Overview
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">-->
            <!-- Youtube embed code here -->
            <!--
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>-->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Panoptic Dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 1 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 2 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 3 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 4 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 5 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/Panoptic 6 Person.mp4"
            type="video/mp4">
          </video>
        </div>
        
      </div>
      <h2 class="subtitle has-text-centered">
        Demo multi-view multi-human 3D tracking and reconstruction on the panoptic dataset, featuring 6 scenes with 1-6 human subjects in each. 
        For each scene, we first present the original videos in 3-4 views, and demonstrate the corresponding 2D reprojections and 3D reconstructions.
      </h2>
    </div>
  </div>
</section>
<!-- End video carousel -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Egohumans Dataset</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/Fencing.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/Lego.mp4"
            type="video/mp4">
          </video>
        </div>
        
      </div>
      <h2 class="subtitle has-text-centered">
        Demo multi-view multi-human 3D tracking and reconstruction on the Egohumans dataset, featuring 2 scenes with 2-3 human subjects in each. 
        For each scene, we first present the original videos in 3-4 views, and demonstrate the corresponding 2D reprojections and 3D reconstructions.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Further Qualitative Results</h2>
      <!--<div class="item">
        <img src="static/images/camera_pos_pred.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Qualitative results on camera pose estimation. The left column visualizes the
ground truth and our estimated camera poses on the Shelf dataset, and the right column
shows the ground truth and our estimated camera poses on the Panoptic dataset.
Results indicate that our method can accurately estimate camera poses.
        </h2>
      </div>-->
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-justified">
          Below is a side-by-side qualitative evaluations of 3D pose tracking performance
          on the CMU Panoptic dataset. The superior consistency of our method is illus-
          trated in the top rows of each scene, where the individual encased in the red bounding
          box consistently retains their color designation, showcasing our method’s robust iden-
          tity tracking over time. In contrast, the bottom rows, representing the PHALP’ results,
          reveal the system’s vulnerability to occlusions, as highlighted by the individual in the
          orange bounding box receiving varied color identities, indicating identity switches.
       </h2>
        <img src="static/images/tracking switch id.png" alt="MY ALT TEXT"/>
     </div>
      <div class="item">
        <!-- Your image here -->
        <h2 class="subtitle has-text-justified">
          All three rows of input
videos below are from Panoptic. Three views are presented for each input video with a
3D reconstruction of human(s) on the right which is shared across all views. Below
each input video row, we demonstrate the 2D reprojections of the 3D reconstruction
for each view.
        </h2>
        <img src="static/images/ECCV Qualitative.jpg" alt="MY ALT TEXT"/>
      </div>
     <div class="item">
      <!-- Your image here -->
      <h2 class="subtitle has-text-justified">
       Below are additional Qualitative results of the proposed approach. All input
videos are from the panoptic dataset. The columns compare results from three
methods: PHALP’, SLAHMR, and EasyRet3D. The examples include different
ranges of people (from N = 1 to N = 6) in a given scene. The examples include
unusual poses, unusual viewpoints, people in close interaction, and severe occlusions.
For each example we show the input image, the reconstruction overlay, a front view
and a additional view. For our method EasyRet3D, we also show the reconstruction
overlay to another view.

      </h2>
      <img src="static/images/Further Qualitative ECCV.jpg" alt="MY ALT TEXT"/>
      
    </div>
  </div>
</div>
</div>



<!-- Paper poster -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>-->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
